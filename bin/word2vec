#!/usr/bin/env node

'use strict';

var parseArgs = require('minimist');
var Word2vec = require('../lib/word2vec');

function main(argv) {

    return Promise.resolve().then(function() {
    
        argv = argv.slice(2);
        if (0 == argv.length) {
            console.log("WORD VECTOR estimation toolkit v 0.1c\n");
            console.log("Options:");
            console.log("Parameters for training:");
            console.log("\t-train <file>");
            console.log("\t\tUse text data from <file> to train the model");
            console.log("\t-output <file>");
            console.log("\t\tUse <file> to save the resulting word vectors / word clusters");
            console.log("\t-size <int>");
            console.log("\t\tSet size of word vectors; default is 100");
            console.log("\t-window <int>");
            console.log("\t\tSet max skip length between words; default is 5");
            console.log("\t-sample <float>");
            console.log("\t\tSet threshold for occurrence of words. Those that appear with higher frequency in the training data");
            console.log("\t\twill be randomly down-sampled; default is 1e-3, useful range is (0, 1e-5)");
            console.log("\t-hs <int>");
            console.log("\t\tUse Hierarchical Softmax; default is 0 (not used)");
            console.log("\t-negative <int>");
            console.log("\t\tNumber of negative examples; default is 5, common values are 3 - 10 (0 = not used)");
            console.log("\t-threads <int>");
            console.log("\t\tUse <int> threads (default 12)");
            console.log("\t-iter <int>");
            console.log("\t\tRun more training iterations (default 5)");
            console.log("\t-min-count <int>");
            console.log("\t\tThis will discard words that appear less than <int> times; default is 5");
            console.log("\t-alpha <float>");
            console.log("\t\tSet the starting learning rate; default is 0.025 for skip-gram and 0.05 for CBOW");
            console.log("\t-classes <int>");
            console.log("\t\tOutput word classes rather than word vectors; default number of classes is 0 (vectors are written)");
            console.log("\t-debug <int>");
            console.log("\t\tSet the debug mode (default = 2 = more info during training)");
            console.log("\t-binary <int>");
            console.log("\t\tSave the resulting vectors in binary moded; default is 0 (off)");
            console.log("\t-save-vocab <file>");
            console.log("\t\tThe vocabulary will be saved to <file>");
            console.log("\t-read-vocab <file>");
            console.log("\t\tThe vocabulary will be read from <file>, not constructed from the training data");
            console.log("\t-cbow <int>");
            console.log("\t\tUse the continuous bag of words model; default is 0 (skip-gram model)");
            console.log("\nExamples:");
            console.log("./word2vec -train data.txt -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 5 -hs 0 -binary 0 -cbow 0 -iter 3\n");
            return 0;
        }
        
        argv = parseArgs(argv);
        
        var params = {};
        params.layer1_size = argv.hasOwnProperty('size') ? parseInt(argv['size']) : 100;
        params.train_file = argv.hasOwnProperty('train') ? argv['train'] : null;
        params.save_vocab_file = argv.hasOwnProperty('save-vocab') ? argv['save-vocab'] : null;
        params.read_vocab_file = argv.hasOwnProperty('read-vocab') ? argv['read-vocab'] : null;
        params.debug_mode = argv.hasOwnProperty('debug') ? parseInt(argv['debug']) : 2;
        params.binary = argv.hasOwnProperty('binary') ? parseInt(argv['binary']) : 0;
        params.cbow = argv.hasOwnProperty('cbow') ? !!parseInt(argv['cbow']) : false;
        params.alpha = argv.hasOwnProperty('alpha') ? parseFloat(argv['alpha']) : (params.cbow ? 0.05 : 0.025);
        params.output_file = argv.hasOwnProperty('output') ? argv['output'] : false;
        params.window = argv.hasOwnProperty('window') ? parseInt(argv['window']) : 5;
        params.sample = argv.hasOwnProperty('sample') ? parseFloat(argv['sample']) : 1e-3;
        params.hs = argv.hasOwnProperty('hs') ? parseInt(argv['hs']) : 0;
        params.negative = argv.hasOwnProperty('negative') ? parseInt(argv['negative']) : 5;
        params.num_threads = argv.hasOwnProperty('threads') ? parseInt(argv['threads']) : 1;
        params.iter = argv.hasOwnProperty('iter') ? parseInt(argv['iter']) : 5;
        params.min_count = argv.hasOwnProperty('min-count') ? parseInt(argv['min-count']) : 5;
        params.classes = argv.hasOwnProperty('classes') ? parseInt(argv['classes']) : 0;
    
        if (params.cbow) {
            throw new Error('CBOW model not implemented');
        }
        if (params.hs) {
            throw new Error('Hierarchical soft-max not implemented');
        }
        if (params.threads > 1) {
            throw new Error('Parallel processing not implemented');
        }
        
        var word2vec = new Word2vec(params);
        return word2vec.trainModel();

    });
    
}

Promise.resolve().then(function() {
    return main(process.argv);
}).then(function(exitCode) {
    process.exit(exitCode);
}).catch(function(err) {
    console.error(err.stack || err);
    process.exit(-1);
});
